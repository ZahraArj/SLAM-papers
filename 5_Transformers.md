<!---
Started to write on Sep 7 2021
Zahra
-->

## [Transformers](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)
- paper: [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
- Seq2Seq architecture like 
- Seq2Seq models consist of an Encoder and a Decoder
- **The output sequence can be in another language, symbols, a copy of the input, etc.**
- It does not imply any Recurrent Networks (GRU, LSTM, etc.).
 
  
 <img width="700" alt="vae-gaussian" src="https://user-images.githubusercontent.com/46463022/132400632-0cb86cc9-1dc6-4753-a6e0-8c42844be46c.png">
  
