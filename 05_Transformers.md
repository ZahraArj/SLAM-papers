<!---
Started to write on Sep 7 2021
Zahra
-->

## [Transformers](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)
- paper: [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
- It does not imply any Recurrent Networks (GRU, LSTM, etc.).
- 1st Task: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.](https://arxiv.org/pdf/1810.04805.pdf)
- 2nd Task:...
 
  
 <img src="https://user-images.githubusercontent.com/46463022/132421458-b69717d0-1aed-4f80-a15d-073cce446cc3.png">

  
